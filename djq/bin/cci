#!/usr/bin/env python2.7
# -*- mode: Python -*-
#
# (C) British Crown Copyright 2018, Met Office.
# See LICENSE.md in the top directory for license details.
#

# Compare CV Implementations: read a request, evaluate it with two
# implementations, and emit a metric for how similar they are.
#
# This originated as a copy of djq, and shares far too much code with
# it.
#

from __future__ import print_function
from sys import stdin, stdout, stderr, argv
from signal import SIGINT
from os import _exit, EX_IOERR
from argparse import ArgumentParser
from importlib import import_module
from json import dump
from djq import read_request, process_request
from djq.low import verbosity_level, mutter, debug_level, debug
from djq.low import InternalException, Scram, Disaster
from djq.low import checks_minpri, checks_enabled
from djq.low import stringlike
from djq.variables import validate_cv_implementation
from djq import __path__ as djq_path

class BotchedReply(Disaster):
    def __init__(self, fmt, *args):
        self.description = "botched reply: {}".format(fmt.format(*args))
    def __str__(self):
        return self.description

class EmitComparisonFailed(InternalException):
    def __init__(self, string, wrapped=None):
        super(EmitComparisonFailed, self).__init__(string)
        self.wrapped = wrapped
    def __str__(self):
        return "{}: {}".format(super(EmitComparisonFailed, self).__str__(),
                               self.wrapped)

def emit_comparison(comparison, output, human=False):
    """Emit the results of a comparison.

    This is related to djq.emit.emit_reply(), but it can also emit
    things as human-readable output.
    """
    try:
        if not human:
            dump(comparison, output, indent=1)
            output.write("\n")
        else:
            for (mip, experiment, similarity) in comparison:
                if experiment is True:
                    experiment = "+"
                elif experiment is None:
                    experiment = "-"
                elif not stringlike(experiment):
                    raise BotchedReply("experiment {} is a {}"
                                       .format( experiment, type(experiment)))
                # These widths are empirically OK.  3 decimal places
                # for the similarity means, say, implementations
                # returning 999 and 1000 results should always be
                # visibly different.
                print >>output, "{:20s} {:30s} {:5.3f}".format(mip,
                                                               experiment,
                                                               similarity)
    except Exception as e:
        raise EmitComparisonFailed("badness when emitting comparison", e)

def compare_replies(r1, r2):
    """for each reply in r1 & r2 produce the comparison.

    r1 & r1 are tuples of replies which must match: each element
    must be for the same mip and experiment or we will doom.

    Return a tuple of comparisons where each element is tuple of (mip,
    experiment, similarity)
    """

    def similarity(s1, s2):
        # How similar are two sets s1 and s2: this is a number between
        # 0.0 (completely different) and 1.0 (identical), which is
        # computed based on the size of the symmetric difference
        # divided by the summed size of the two sets.
        l = len(s1) + len(s2)
        if l == 0:
            # empty sets are identical
            return 1.0
        else:
            return 1.0 - (1.0 * len(s1 ^ s2))/l

    def compare_single_replies(sr1, sr2):
        debug("{} {} / {} {}".format(sr1['mip'], sr1['experiment'],
                                     sr2['mip'], sr2['experiment']))
        if sr1['mip'] != sr2['mip']:
            raise BotchedReply("mips {} & {} are different",
                               sr1['mip'], sr2['mip'])
        if sr1['experiment'] != sr2['experiment']:
            raise BotchedReply("experiments {} & {} are different in {}",
                               sr1['experiment'], sr2['experiment'],
                               sr1['mip'])
        return (sr1['mip'],
                sr1['experiment'],
                similarity(set(v['label']
                               for v in (sr1['reply-variables']
                                         if sr1['reply-variables'] is not None
                                         else ())),
                           set(v['label']
                               for v in (sr2['reply-variables']
                                         if sr2['reply-variables'] is not None
                                         else ()))))

    if len(r1) != len(r2):
        raise BotchedReply("reply lengths {} & {} aren't the same",
                           len(r1), len(r2))

    return(tuple(compare_single_replies(sr1, sr2)
                 for (sr1, sr2) in zip(r1, r2)))

def stream_compare_implementations(i1, i2, input, output,
                                   human=False,
                                   **pr_kws):
    rq = read_request(input)
    emit_comparison(compare_replies(process_request(rq, cvimpl=i1, **pr_kws),
                                    process_request(rq, cvimpl=i2, **pr_kws)),
                    output, human=human)

def main():
    early = True                # nothing is sane until this is false
    parser = ArgumentParser(description="djq implementation comparison")
    parser.add_argument("-r", "--root",
                        default=None, dest='dqroot',
                        help="the top of a checked out CMIP6 DREQ")
    parser.add_argument("-t", "--tag",
                        default=None, dest='dqtag',
                        help="the default tag for the DREQ")
    parser.add_argument("-u", "--use-trunk",
                        action='store_false', dest='dqtag',
                        help="use the DREQ trunk instead of a tag")
    parser.add_argument("-p", "--path-to-xml-directory",
                        default=None, dest='dqpath',
                        help="Directory containing the XML files")
    parser.add_argument("-j", "--jsonify-implementation",
                        default=None, dest='jsonify_implementation',
                        help="the name of a JSONify implementation to load")
    parser.add_argument("-f", "--feature-bundle",
                        default=None, dest='fbundle',
                        help="the name of a JSON feature bundle file")
    parser.add_argument("-v", "--verbose",
                        action='count', dest='verbosity',
                        help="increase verbosity (repeat for more noise)")
    parser.add_argument("-d", "--debug",
                        action='count', dest='debug',
                        help="debugging output (repeat for, perhaps, more)")
    parser.add_argument("-b", "--backtrace",
                        action='store_true', dest='backtrace',
                        help="don't suppress backtraces")
    parser.add_argument("-c", "--check-priority",
                        action='store', type=int,
                        dest='check_priority', default=0,
                        help="set the lowest check priority that will run")
    parser.add_argument("-o", "--output",
                        default=None, dest='output',
                        help="output file (stdout default)")
    parser.add_argument("-s", "--simple-output",
                        action='store_true', dest='human',
                        help="write human-readable output, not JSON")
    parser.add_argument("-1", "--implementation-1", dest='i1',
                        default="djq.variables.cv_default",
                        help="the name of the first implementation module")
    parser.add_argument("-2", "--implementation-2", dest='i2',
                        default="djq.variables.cv_default",
                        help="the name of the second implementation module")
    parser.add_argument('request', nargs='?', default=None,
                        help="JSON request (stdin default)")
    backtrace = None
    try:
        # Some things need to be set up right away, but only set up
        # what we must set up.
        args = parser.parse_args()
        early = False
        debug_level(args.debug)            # must set this now
        verbosity_level(args.verbosity)    # also
        checks_minpri(args.check_priority) # no argument for this
        debug("cci from {}", djq_path[0])
        debug("checks {} minpri {}", checks_enabled(), checks_minpri())
        mutter("from {} to {}",
               (args.request if args.request is not None else "-"),
               (args.output if args.output is not None else "-"))
        i1 = validate_cv_implementation(import_module(args.i1))
        i2 = validate_cv_implementation(import_module(args.i2))
        js = (import_module(args.jsonify_implementation)
              if args.jsonify_implementation is not None
              else None)
        with (open(args.request)
              if args.request is not None
              else stdin) as input, (open(args.output, 'w')
                                     if args.output is not None
                                     else stdout) as output:
            stream_compare_implementations(i1, i2, input, output,
                                           human=args.human,
                                           dqroot=args.dqroot,
                                           dqtag=args.dqtag,
                                           dqpath=args.dqpath,
                                           dq=None,
                                           dbg=args.debug,           # rebind
                                           verbosity=args.verbosity, # also
                                           jsimpl=js,
                                           fbundle=args.fbundle)
    except Scram as e:
        raise
    except Exception as e:
        if early:
            # something has gone wrong before anything is set up
            print("{}: very early badness".format(argv[0]), file=stderr)
            raise
        elif args.backtrace:
            raise
        else:
            exit(e)
    except KeyboardInterrupt as e:
        exit(128 + SIGINT)

    # Try and evade the 'lost stderr' spurions by just bottling out
    #
    try:
        if not stdout.closed:
            stdout.flush()
            stdout.close()
        if not stderr.closed:
            stderr.flush()
            stderr.close()
    except:
        if debug_level() < 2:
            _exit(EX_IOERR)
        else:
            raise

if __name__ == '__main__':
    main()
